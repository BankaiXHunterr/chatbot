from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import logging
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import google.generativeai as genai
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

load_dotenv()
os.getenv('GOOGLE_API_KEY')
genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))


class ConversationHandler:
    def __init__(self):
        """
        Initializes the ConversationHandler object.
        Sets up the necessary attributes such as embeddings, generative model, and prompt template.
        """
        self.embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        self.model = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.3)
        self.prompt_template = """
        Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in
        provided context just say, "answer is not available in the context", don't provide the wrong answer\n\n
        Context:\n {context}?\n
        Question: \n{question}\n

        Answer:
        """

    async def get_text_chunks(self, text):
        """
        Splits a given text string into manageable chunks for further processing.

        Args:
            text (str): The text content to be split into chunks.

        Returns:
            List[str]: A list containing the split text chunks.
        """
        try:
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)
            chunks = text_splitter.split_text(text)
            return chunks
        except Exception as e:
            raise Exception("Error in get_text_chunks: {}".format(str(e)))

    async def get_vector_store(self, text_chunks):
        """
        Creates a FAISS index for efficient text retrieval using pre-trained embeddings.

        Args:
            text_chunks (List[str]): A list of text chunks obtained from splitting the original text.
        """
        try:
            vector_store = FAISS.from_texts(text_chunks, embedding=self.embeddings)
            vector_store.save_local("faiss_index")
        except Exception as e:
            raise Exception("Error in get_vector_store: {}".format(str(e)))

    async def get_conversational_chain(self):
        """
        Retrieves a conversational chain (sequence of question-answer pairs) using a generative AI model.

        Returns:
            List: A list containing dictionaries representing the conversational chain.
        """
        try:
            prompt = PromptTemplate(template=self.prompt_template, input_variables=["context", "question"])
            chain = load_qa_chain(self.model, chain_type="stuff", prompt=prompt)
            return chain
        except Exception as e:
            raise Exception("Error in get_conversational_chain: {}".format(str(e)))

    async def user_input(self, user_question):
        """
        Handles user input and generates a conversational response.

        Args:
            user_question (str): The question posed by the user.

        Returns:
            str: The response generated by the conversational model.
        """
        try:
            new_db = FAISS.load_local("faiss_index", self.embeddings, allow_dangerous_deserialization=True)
            docs = new_db.similarity_search(user_question)
            chain = await self.get_conversational_chain()
            response = chain({"input_documents": docs, "question": user_question}, return_only_outputs=True)
            print(response)
            return response["output_text"]
        except Exception as e:
            raise Exception("Error in user_input: {}".format(str(e)))
